{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3461004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.10\n",
      "  Using cached tensorflow-2.10.0-cp310-cp310-win_amd64.whl (455.9 MB)\n",
      "Collecting tensorflow-gpu==2.10\n",
      "  Using cached tensorflow_gpu-2.10.0-cp310-cp310-win_amd64.whl (455.9 MB)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-win_amd64.whl (38.2 MB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.7.1-cp310-cp310-win_amd64.whl (7.6 MB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.10)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.10)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow==2.10)\n",
      "  Using cached flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.10)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.10)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.10)\n",
      "  Using cached h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.10)\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.10)\n",
      "  Using cached libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting numpy>=1.20 (from tensorflow==2.10)\n",
      "  Using cached numpy-1.24.3-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.10)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in d:\\projects\\toxic comment classification\\toxic\\lib\\site-packages (from tensorflow==2.10) (23.1)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.10)\n",
      "  Using cached protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "Requirement already satisfied: setuptools in d:\\projects\\toxic comment classification\\toxic\\lib\\site-packages (from tensorflow==2.10) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\projects\\toxic comment classification\\toxic\\lib\\site-packages (from tensorflow==2.10) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.10)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow==2.10)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.10)\n",
      "  Using cached wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.10)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.10)\n",
      "  Using cached grpcio-1.54.0-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "Collecting tensorboard<2.11,>=2.10 (from tensorflow==2.10)\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0 (from tensorflow==2.10)\n",
      "  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting keras<2.11,>=2.10.0 (from tensorflow==2.10)\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.0.7-cp310-cp310-win_amd64.whl (162 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.39.3-py3-none-any.whl (1.0 MB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-win_amd64.whl (55 kB)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Using cached Pillow-9.5.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\projects\\toxic comment classification\\toxic\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.10)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Downloading Werkzeug-2.3.0-py3-none-any.whl (233 kB)\n",
      "                                              0.0/233.7 kB ? eta -:--:--\n",
      "     ----------                              61.4/233.7 kB 1.6 MB/s eta 0:00:01\n",
      "     ------------------------------------   225.3/233.7 kB 2.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 233.7/233.7 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached charset_normalizer-3.1.0-cp310-cp310-win_amd64.whl (97 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-win_amd64.whl (16 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, keras, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyparsing, pyasn1, protobuf, pillow, oauthlib, numpy, MarkupSafe, markdown, kiwisolver, idna, grpcio, google-pasta, gast, fonttools, cycler, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, opencv-python, keras-preprocessing, h5py, contourpy, astunparse, requests-oauthlib, matplotlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-gpu, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2022.12.7 charset-normalizer-3.1.0 contourpy-1.0.7 cycler-0.11.0 flatbuffers-23.3.3 fonttools-4.39.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.54.0 h5py-3.8.0 idna-3.4 keras-2.10.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 libclang-16.0.0 markdown-3.4.3 matplotlib-3.7.1 numpy-1.24.3 oauthlib-3.2.2 opencv-python-4.7.0.72 opt-einsum-3.3.0 pillow-9.5.0 protobuf-3.19.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 pyparsing-3.0.9 requests-2.28.2 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-gpu-2.10.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.15 werkzeug-2.3.0 wheel-0.40.0 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.10 tensorflow-gpu==2.10 opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf2a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d804efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b62df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e164609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bebc0d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84e60840",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(os.path.join('data','train.csv','train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b22e2c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic   \n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0  \\\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65889c2b",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "514441c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a25be60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c33e0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77e15930",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES=200000 # number of words in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1c55210",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TextVectorization(max_tokens=MAX_FEATURES,\n",
    "                             output_sequence_length=1800,\n",
    "                             output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae761748",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.adapt(X.values) #passing X pandas series as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10ccde70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       "       \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       ...,\n",
       "       'Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.',\n",
       "       'And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.',\n",
       "       '\"\\nAnd ... I really don\\'t think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a1c3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60972521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(159571, 1800), dtype=int64, numpy=\n",
       "array([[  643,    76,     2, ...,     0,     0,     0],\n",
       "       [    1,    54,  2506, ...,     0,     0,     0],\n",
       "       [  425,   440,    70, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [32141,  7329,   383, ...,     0,     0,     0],\n",
       "       [    5,    12,   533, ...,     0,     0,     0],\n",
       "       [    5,     8,   130, ...,     0,     0,     0]], dtype=int64)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5fadade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e397f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow pipeline\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text,y))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(160000)\n",
    "dataset = dataset.batch(16)\n",
    "dataset = dataset.prefetch(8) #helps prevent bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8a2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd11dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2e2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3822c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061eca4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "toxic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
